---
title: "A Statistical Critique of Wildfire Reporting in Canadian Media"
subtitle: "Reproducible Analysis Code"
author: "Hanniel Kouame"
date: "2025-10-20"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  dpi = 300
)
```

# Introduction

This document contains all code necessary to reproduce the statistical analysis presented in "A Statistical Critique of Wildfire Reporting in Canadian Media."

**Research Questions Addressed:**

1. **Denley (Ottawa Citizen, 2023)**: Have Canadian fire frequency and area burned declined since 1989-1995? Do temperature and fire activity move in opposite directions?

2. **Sankey (National Post, 2023)**: Has the proportion of human-caused fires been increasing, potentially explaining recent fire trends independent of climate factors?

3. **Lomborg (National Post, 2024)**: Are global fire trends declining when viewed in appropriate temporal context, and are North American increases offset by decreases elsewhere?

# Setup and Dependencies

## Load Required Packages

```{r load-packages, message=FALSE, warning=FALSE}
# Data manipulation
library(tidyverse)
library(readr)
library(lubridate)
library(zoo)

# Statistical analysis
library(Kendall)      # Mann-Kendall tests
library(trend)        # Sen's slope
library(FSA)          # Dunn's test
library(lmtest)       # Durbin-Watson test
library(car)          # Levene's test
library(broom)        # Tidy model outputs

# Visualization
library(ggplot2)
library(ggpubr)       # Publication plots with stats
library(scales)
library(patchwork)

# Create directories for outputs if they don't exist
# These will be created in your current working directory
dir.create("results", showWarnings = FALSE, recursive = TRUE)
dir.create("figures", showWarnings = FALSE, recursive = TRUE)
dir.create("figures/hypothesis_tests", showWarnings = FALSE, recursive = TRUE)
dir.create("data_processed", showWarnings = FALSE, recursive = TRUE)
```

## Session Information

```{r session-info}
sessionInfo()
```

---

# Data Preparation

## Instructions for Data Acquisition

Before running this code, you need to download the following datasets:

1. **Canadian National Fire Database (CNFDB)**
   - Source: http://nfdp.ccfm.org/en/data/fires.php
   - Files needed:
     - `CNFDB_area_by_cause.csv`
     - `CNFDB_fires_by_cause.csv`
     - `CNFDB_fires_by_size.csv`

2. **Berkeley Earth Surface Temperature**
   - Source: http://berkeleyearth.org/data/
   - File needed: Canada temperature data

3. **Global Wildfire Information System (GWIS)**
   - Source: https://gwis.jrc.ec.europa.eu/apps/gwis.statistics/seasonaltrend
   - File needed: Global burned area data (2002-2023)

**Place all raw data files in a folder named `data/` in your working directory.**

## 1. Canadian National Fire Database (CNFDB)

### Data Cleaning Function

```{r cnfdb-cleaning-function}
clean_cnfdb_data <- function(input_file, output_file, value_column_name) {
  #' Clean and reshape CNFDB wide-format data to long format
  #'
  #' @param input_file Path to input CSV file (UTF-16LE encoded)
  #' @param output_file Path to output CSV file
  #' @param value_column_name Name for the value column (e.g., 'Area_ha', 'Fire_Count')
  #' @return A cleaned and reshaped tibble
  
  # Read the CSV with UTF-16LE encoding
  df <- read_delim(input_file, 
                   delim = "\t",
                   locale = locale(encoding = "UTF-16LE"),
                   show_col_types = FALSE)
  
  # Remove BOM and clean column names
  colnames(df) <- str_replace_all(colnames(df), "^\uFEFF", "")
  colnames(df) <- str_trim(colnames(df))
  
  # The first row contains the actual column names
  new_columns <- as.character(df[1, ])
  df <- df[-1, ]
  
  # Set proper column names
  colnames(df) <- new_columns
  
  # Identify ID columns (first 3) and year columns (rest)
  id_columns <- colnames(df)[1:3]
  year_columns <- colnames(df)[4:ncol(df)]
  
  # Clean ID columns - trim whitespace
  df <- df %>%
    mutate(across(all_of(id_columns), str_trim))
  
  # Reshape from wide to long format
  df_long <- df %>%
    pivot_longer(
      cols = all_of(year_columns),
      names_to = "Year",
      values_to = value_column_name
    )
  
  # Clean the Year column - convert to integer
  df_long <- df_long %>%
    mutate(Year = as.integer(Year))
  
  # Clean the value column - remove commas and convert to numeric
  df_long <- df_long %>%
    mutate(!!sym(value_column_name) := as.numeric(str_replace_all(.data[[value_column_name]], ",", "")))
  
  # Remove rows where Year is NA
  df_long <- df_long %>%
    filter(!is.na(Year))
  
  # Sort by Jurisdiction, Year, and the second ID column
  df_long <- df_long %>%
    arrange(!!sym(id_columns[1]), Year, !!sym(id_columns[2]))
  
  # Save to CSV
  write_csv(df_long, output_file)
  
  # Print summary
  cat(sprintf("  Processed %s\n", input_file))
  cat(sprintf("  - Original shape: %d rows × %d columns\n", nrow(df), ncol(df)))
  cat(sprintf("  - Reshaped to: %d rows × %d columns\n", nrow(df_long), ncol(df_long)))
  cat(sprintf("  - Saved to: %s\n", output_file))
  cat(sprintf("  - Years range: %d to %d\n\n", min(df_long$Year, na.rm = TRUE), max(df_long$Year, na.rm = TRUE)))
  
  return(df_long)
}

cat(paste0(rep("=", 60), collapse = ""), "\n")
cat("CNFDB Data Cleaning and Reshaping\n")
cat(paste0(rep("=", 60), collapse = ""), "\n\n")
```

### Process CNFDB Files

```{r process-cnfdb, eval=FALSE}
# NOTE: Set eval=TRUE after you've placed the data files in a data/ folder

# Process the three CNFDB datasets
# 1. Area by Cause
df_area <- clean_cnfdb_data(
  input_file = 'data/CNFDB_area_by_cause.csv',
  output_file = 'data_processed/CNFDB_area_by_cause_clean.csv',
  value_column_name = 'Area_ha'
)

# 2. Fires by Cause
df_fires_cause <- clean_cnfdb_data(
  input_file = 'data/CNFDB_fires_by_cause.csv',
  output_file = 'data_processed/CNFDB_fires_by_cause_clean.csv',
  value_column_name = 'Fire_Count'
)

# 3. Fires by Size
df_fires_size <- clean_cnfdb_data(
  input_file = 'data/CNFDB_fires_by_size.csv',
  output_file = 'data_processed/CNFDB_fires_by_size_clean.csv',
  value_column_name = 'Fire_Count'
)
```

### Load Cleaned CNFDB Data

```{r load-cnfdb}
# Load the cleaned datasets
# If you haven't run the cleaning step, you'll need to do so first

df_area <- read_csv('data_processed/CNFDB_area_by_cause_clean.csv', show_col_types = FALSE)
df_fires_cause <- read_csv('data_processed/CNFDB_fires_by_cause_clean.csv', show_col_types = FALSE)
df_fires_size <- read_csv('data_processed/CNFDB_fires_by_size_clean.csv', show_col_types = FALSE)

# Display structure
cat("Area by Cause Data:\n")
str(df_area)
cat("\nFires by Cause Data:\n")
str(df_fires_cause)
cat("\nFires by Size Data:\n")
str(df_fires_size)
```
```{r filter-data-qualifier}
cat("========================================\n")
cat("DATA QUALIFIER FILTERING\n")
cat("========================================\n\n")

cat("Before filtering:\n")
cat("  Area records:", nrow(df_area), "\n")
cat("  Fire records:", nrow(df_fires_cause), "\n\n")

# Check if Data Qualifier column exists
if ("Data_Qualifier" %in% colnames(df_area) | "Data Qualifier" %in% colnames(df_area)) {
  
  # Standardize column name
  if ("Data Qualifier" %in% colnames(df_area)) {
    df_area <- df_area %>% rename(Data_Qualifier = `Data Qualifier`)
    df_fires_cause <- df_fires_cause %>% rename(Data_Qualifier = `Data Qualifier`)
    df_fires_size <- df_fires_size %>% rename(Data_Qualifier = `Data Qualifier`)
  }
  
  # Filter: qualifier "a" for 1990-2022, allow "e" for 2023-2024
  df_area <- df_area %>%
    filter(Data_Qualifier == "a" | (Data_Qualifier == "e" & Year >= 2023))
  
  df_fires_cause <- df_fires_cause %>%
    filter(Data_Qualifier == "a" | (Data_Qualifier == "e" & Year >= 2023))
  
  df_fires_size <- df_fires_size %>%
    filter(Data_Qualifier == "a" | (Data_Qualifier == "e" & Year >= 2023))
  
  cat("After filtering:\n")
  cat("  Area records:", nrow(df_area), "\n")
  cat("  Fire records:", nrow(df_fires_cause), "\n\n")
  
  cat("Filter applied:\n")
  cat("  Years 1990-2022: Only qualifier 'a' (actual reported data)\n")
  cat("  Years 2023-2024: Qualifier 'a' OR 'e' (agency estimates included)\n\n")
  
} else {
  cat("WARNING: Data_Qualifier column not found!\n")
  cat("Proceeding without filtering - this may affect results.\n\n")
  cat("If your raw data has a 'Data Qualifier' column, make sure\n")
  cat("the clean_cnfdb_data() function preserves it.\n\n")
}
```

### Missing Data Analysis

```{r missing-data-analysis}
cat("========================================\n")
cat("STEP 3: MISSING DATA ANALYSIS\n")
cat("========================================\n\n")

cat("METHODOLOGY VERIFICATION #3: Missing Area Values\n")


# Calculate missing percentages
missing_area_count <- sum(is.na(df_area$Area_ha))
missing_area_pct <- (missing_area_count / nrow(df_area)) * 100

# Also check for zeros (which may represent missing values treated as 0)
zero_area_count <- sum(df_area$Area_ha == 0, na.rm = TRUE)
zero_area_pct <- (zero_area_count / nrow(df_area)) * 100

cat("Missing data statistics:\n")
cat(sprintf("  Records with NA area: %s (%.1f%%)\n", 
            format(missing_area_count, big.mark = ","), 
            missing_area_pct))
cat(sprintf("  Records with 0 area: %s (%.1f%%)\n", 
            format(zero_area_count, big.mark = ","), 
            zero_area_pct))
cat(sprintf("  Total missing or zero: %.1f%%\n\n", 
            missing_area_pct + zero_area_pct))

cat("METHODOLOGY VERIFICATION #4: Treatment of Missing Values\n")
cat("  Implementation: Using na.rm = TRUE in sum() operations\n")
cat("                  Effect: NA values are ignored, equivalent to treating as 0\n\n")

# Show example
cat("Example: Total area with NA values\n")
example_with_na <- df_area %>% 
  filter(Year == 2020) %>%
  summarise(
    Total_with_naRM = sum(Area_ha, na.rm = TRUE),
    Total_without_naRM = sum(Area_ha, na.rm = FALSE)
  )
cat("  sum(Area_ha, na.rm = TRUE):", format(example_with_na$Total_with_naRM, big.mark = ","), "ha\n")
cat("  sum(Area_ha, na.rm = FALSE):", 
    ifelse(is.na(example_with_na$Total_without_naRM), "NA (fails if any NA present)", 
           format(example_with_na$Total_without_naRM, big.mark = ",")), "\n\n")

cat("Missing values will be treated as zeros in all aggregations\n\n")
```

### Reclassify Fire Causes (6 -> 3 Categories)

```{r reclassify-causes}
cat("========================================\n")
cat("FIRE CAUSE RECLASSIFICATION\n")
cat("========================================\n\n")

cat("Original categories in data:\n")
print(sort(unique(df_area$Cause)))
cat("\n")

# Reclassify area data
df_area <- df_area %>%
  mutate(Cause_Simplified = case_when(
    Cause %in% c("Human activity", "Prescribed burn") ~ "Human",
    Cause %in% c("Lightning", "Natural cause") ~ "Lightning",
    Cause %in% c("Unspecified", "Reburn") ~ "Unknown",
    TRUE ~ "Other"
  ))

# Reclassify fire count data
df_fires_cause <- df_fires_cause %>%
  mutate(Cause_Simplified = case_when(
    Cause %in% c("Human activity", "Prescribed burn") ~ "Human",
    Cause %in% c("Lightning", "Natural cause") ~ "Lightning",
    Cause %in% c("Unspecified", "Reburn") ~ "Unknown",
    TRUE ~ "Other"
  ))

cat("Reclassification complete:\n")
cat("  Human activity + Prescribed burn → Human\n")
cat("  Lightning + Natural cause → Lightning\n")
cat("  Unspecified + Reburn → Unknown\n\n")

cat("New categories:\n")
print(sort(unique(df_area$Cause_Simplified)))
cat("\n")
```

### Aggregate to National Level

```{r create-national-aggregates}
# Aggregate by simplified cause
canada_area <- df_area %>%
  group_by(Year, Cause_Simplified) %>%
  summarise(Area_ha = sum(Area_ha, na.rm = TRUE), .groups = 'drop') %>%
  rename(Cause = Cause_Simplified)

canada_fires_cause <- df_fires_cause %>%
  group_by(Year, Cause_Simplified) %>%
  summarise(Fire_Count = sum(Fire_Count, na.rm = TRUE), .groups = 'drop') %>%
  rename(Cause = Cause_Simplified)

canada_fires_size <- df_fires_size %>%
  rename(Fire_size_class = `Fire size class`) %>%
  group_by(Year, Fire_size_class) %>%
  summarise(Fire_Count = sum(Fire_Count, na.rm = TRUE), .groups = 'drop')

# Total annual fires and area
canada_annual <- df_area %>%
  group_by(Year) %>%
  summarise(Total_Area_ha = sum(Area_ha, na.rm = TRUE), .groups = 'drop') %>%
  left_join(
    df_fires_cause %>%
      group_by(Year) %>%
      summarise(Total_Fires = sum(Fire_Count, na.rm = TRUE), .groups = 'drop'),
    by = "Year"
  )

cat("National aggregates created\n")
cat("  Years:", min(canada_annual$Year), "-", max(canada_annual$Year), "\n")
cat("  Causes:", paste(sort(unique(canada_area$Cause)), collapse = ", "), "\n\n")

# Check 2023 data
cat("2023 Data Distribution by Cause:\n")
canada_area %>%
  filter(Year == 2023) %>%
  arrange(Cause) %>%
  mutate(Percent = Area_ha / sum(Area_ha) * 100) %>%
  print()
cat("\n")

# Save processed data
write_csv(canada_area, "data_processed/canada_area_by_cause.csv")
write_csv(canada_fires_cause, "data_processed/canada_fires_by_cause.csv")
write_csv(canada_fires_size, "data_processed/canada_fires_by_size.csv")
write_csv(canada_annual, "data_processed/canada_annual_totals.csv")

cat("National aggregates created and saved.\n")
head(canada_annual, 10)
```

## 2. Berkeley Earth Temperature Data

```{r load-temperature, eval=FALSE}
# NOTE: Update the file path to match your temperature data file location
# Set eval=TRUE after placing the file in data/

temp_data <- read_csv('data/canada.csv', show_col_types = FALSE, comment = "#")

baseline_temp <- canada_temp %>%
    filter(year >= 1961 & year <= 1990) %>%
    summarize(baseline_mean = mean(temperature_C, na.rm = TRUE)) %>%
    pull(baseline_mean)

# Process temperature data
canada_temp_annual <- canada_temp %>%
    group_by(year) %>%
    summarize(
      Temp_Annual_Mean = mean(temperature_C, na.rm = TRUE),
      Temp_Annual_Min = min(temperature_C, na.rm = TRUE),
      Temp_Annual_Max = max(temperature_C, na.rm = TRUE),
      Temp_Annual_Uncertainty = mean(uncertainty_C, na.rm = TRUE),
      n_months = n(),
      .groups = "drop"
    ) %>%
    mutate(
      Temp_Annual_Anomaly = Temp_Annual_Mean - baseline_temp
    ) %>%
    rename(Year = year)
  
  # 5. Filter to analysis period and simplify columns
  temp_canada <- canada_temp_annual %>%
    filter(Year >= 1990, Year <= 2020) %>%
    select(Year, Temp_C = Temp_Annual_Mean, Temp_Anomaly = Temp_Annual_Anomaly)
  

write_csv(temp_canada, "data_processed/canada_temperature_annual.csv")


```

```{r load-processed-temperature}
# Load processed temperature data
temp_canada <- read_csv("data_processed/canada_temperature_annual.csv", show_col_types = FALSE)

cat("Temperature data loaded:\n")
head(temp_canada)
```

## 3. Global Wildfire Information System (GWIS)

```{r load-gwis, eval=FALSE}
# NOTE: Update file path as needed
# Set eval=TRUE after placing the file in data/

gwis_data <- read_csv('data/GWIS_global_monthly_burned_area.csv', show_col_types = FALSE)

# Define comprehensive country-to-region mapping
  country_regions <- tribble(
    ~pattern, ~region,
    # North America
    "Canada|United States|USA|Mexico|Greenland|Bermuda|Saint Pierre", "North America",
    
    # Central America & Caribbean
    "Guatemala|Belize|Honduras|El Salvador|Nicaragua|Costa Rica|Panama", "Central America",
    "Cuba|Haiti|Dominican Republic|Jamaica|Trinidad|Bahamas|Barbados", "Caribbean",
    "Puerto Rico|Guadeloupe|Martinique|Aruba|Cayman|Virgin Islands", "Caribbean",
    
    # South America
    "Brazil|Argentina|Colombia|Peru|Venezuela|Chile|Ecuador|Bolivia", "South America",
    "Paraguay|Uruguay|Guyana|Suriname|French Guiana", "South America",
    
    # Europe
    "United Kingdom|UK|France|Germany|Italy|Spain|Portugal|Netherlands", "Europe",
    "Belgium|Austria|Switzerland|Sweden|Norway|Denmark|Finland|Iceland", "Europe",
    "Poland|Czech|Slovakia|Hungary|Romania|Bulgaria|Greece|Croatia", "Europe",
    "Serbia|Bosnia|Albania|North Macedonia|Slovenia|Estonia|Latvia", "Europe",
    "Lithuania|Ireland|Luxembourg|Malta|Cyprus|Monaco|Liechtenstein", "Europe",
    
    # Russia & Central Asia
    "Russia|Russian Federation|Kazakhstan|Uzbekistan|Turkmenistan", "Russia & Central Asia",
    "Kyrgyzstan|Tajikistan|Mongolia|Armenia|Azerbaijan|Georgia", "Russia & Central Asia",
    
    # Middle East & North Africa
    "Turkey|Iran|Iraq|Saudi Arabia|Yemen|Syria|Jordan|Lebanon|Israel", "Middle East & North Africa",
    "Egypt|Libya|Tunisia|Algeria|Morocco|Sudan|United Arab Emirates", "Middle East & North Africa",
    "Kuwait|Qatar|Bahrain|Oman|Palestine|Western Sahara", "Middle East & North Africa",
    
    # Sub-Saharan Africa
    "Nigeria|Ethiopia|Kenya|Tanzania|Uganda|Ghana|Mozambique|Madagascar", "Sub-Saharan Africa",
    "Cameroon|Niger|Mali|Burkina Faso|Malawi|Zambia|Somalia|Senegal", "Sub-Saharan Africa",
    "Chad|Zimbabwe|Guinea|Rwanda|Benin|Burundi|Tunisia|Sierra Leone", "Sub-Saharan Africa",
    "Togo|Libya|Liberia|Mauritania|Eritrea|Gambia|Botswana|Namibia", "Sub-Saharan Africa",
    "Gabon|Lesotho|Guinea-Bissau|Equatorial Guinea|Mauritius|Eswatini", "Sub-Saharan Africa",
    "Djibouti|Comoros|Cape Verde|São Tomé|Seychelles|Angola|Congo", "Sub-Saharan Africa",
    "South Africa|Ivory Coast|Côte d'Ivoire", "Sub-Saharan Africa",
    
    # South Asia
    "India|Pakistan|Bangladesh|Afghanistan|Nepal|Sri Lanka|Bhutan|Maldives", "South Asia",
    
    # East Asia
    "China|Japan|South Korea|North Korea|Taiwan|Hong Kong|Macau", "East Asia",
    
    # Southeast Asia
    "Indonesia|Philippines|Vietnam|Thailand|Myanmar|Malaysia|Cambodia", "Southeast Asia",
    "Laos|Singapore|Timor-Leste|Brunei", "Southeast Asia",
    
    # Oceania
    "Australia|New Zealand|Papua New Guinea|Fiji|Solomon Islands", "Oceania",
    "Vanuatu|Samoa|Kiribati|Tonga|Palau|Marshall Islands|Nauru", "Oceania",
    "Micronesia|Tuvalu|New Caledonia|French Polynesia", "Oceania"
  )

# Identify and clean column names
# Identify key columns
  country_cols <- colnames(gwis_raw)[grepl("country|nation|admin", colnames(gwis_raw), ignore.case = TRUE)]
  year_cols <- colnames(gwis_raw)[grepl("year|time|date", colnames(gwis_raw), ignore.case = TRUE)]
  month_cols <- colnames(gwis_raw)[grepl("month", colnames(gwis_raw), ignore.case = TRUE)]
  landcover_cols <- colnames(gwis_raw)[grepl("forest|savanna|shrub|grass|crop|other", colnames(gwis_raw), ignore.case = TRUE)]
  
  # Select the best match
  country_col <- if(length(country_cols) > 0) country_cols[1] else NULL
  year_col <- if(length(year_cols) > 0) year_cols[1] else NULL
  month_col <- if(length(month_cols) > 0) month_cols[1] else NULL

#Clean and prepare data
gwis_clean <- gwis_raw %>%
    mutate(across(all_of(landcover_cols), as.numeric)) %>%
    mutate(
      burned_area_ha = rowSums(select(., all_of(landcover_cols)), na.rm = TRUE),
      country = str_trim(!!sym(country_col)),
      country = str_replace_all(country, "\\s+", " "),
      year = as.numeric(!!sym(year_col))
    ) %>%
    filter(!is.na(year), burned_area_ha > 0)

# Assign regions to countries
assign_region <- function(country_name) {
    for (i in 1:nrow(country_regions)) {
      if (grepl(country_regions$pattern[i], country_name, ignore.case = TRUE)) {
        return(country_regions$region[i])
      }
    }
    return("Other/Unknown")
  }
  
  gwis_with_regions <- gwis_clean %>%
    mutate(region = map_chr(country, assign_region))
  # Show region distribution
  cat("\nCountries by region:\n")
  region_summary <- gwis_with_regions %>%
    distinct(country, region) %>%
    count(region, name = "n_countries") %>%
    arrange(desc(n_countries))
  print(region_summary)
  cat("\n")
  
  # Check for unmapped countries
  unmapped <- gwis_with_regions %>%
    filter(region == "Other/Unknown") %>%
    distinct(country) %>%
    pull(country)
  
  if (length(unmapped) > 0) {
    cat("Unmapped countries/regions:\n")
    cat(paste("  -", head(unmapped, 10), collapse = "\n"), "\n")
    cat(sprintf("\n  Total unmapped: %d (check region mapping if needed)\n\n", length(unmapped)))
  }
  
# Create aggregated datasets
  # 6A: Annual totals by region
  cat("  Creating annual totals by region...\n")
  gwis_regional <- gwis_with_regions %>%
    group_by(region, year) %>%
    summarise(
      burned_area_ha = sum(burned_area_ha, na.rm = TRUE),
      n_countries = n_distinct(country),
      .groups = 'drop'
    ) %>%
    arrange(region, year)
  
  # 6B: Global annual totals
  cat("  Creating global annual totals...\n")
  gwis_global <- gwis_with_regions %>%
    group_by(year) %>%
    summarise(
      burned_area_ha = sum(burned_area_ha, na.rm = TRUE),
      n_countries = n_distinct(country),
      n_regions = n_distinct(region),
      .groups = 'drop'
    ) %>%
    arrange(year)
  
# Filter to analysis period and save
 gwis_global <- gwis_global %>%
    filter(year >= 2002, year <= 2023)
  
  gwis_regional <- gwis_regional %>%
    filter(year >= 2002, year <= 2023)
  
  # Save processed data
write_csv(gwis_global, "data_processed/gwis_global_annual.csv")
write_csv(gwis_regional, "data_processed/gwis_regional_annual.csv")
```



```{r load-processed-gwis}
# Load processed GWIS data
gwis_global <- read_csv("data_processed/gwis_global_annual.csv", show_col_types = FALSE)
gwis_regional <- read_csv("data_processed/gwis_regional_annual.csv", show_col_types = FALSE)

cat("GWIS data loaded:\n")
head(gwis_global)
```

---

# Analysis: Denley's Article

## Research Question 1: Has wildfire frequency trended down since 1990?

**Claim**: "Since that 1989 peak, and another big year in 1995, wildfire frequency and total area burned have trended down"

- "H0:No trend in fire count over time($\tau$=0)
-  H1: Significant trend exists ($\tau$ $\neq$ 0)

```{r denley-rq1-frequency}
cat("\n========== DENLEY RQ1: Fire Frequency Trend ==========\n\n")

# Prepare data (1990-2023)
fires_annual <- canada_annual %>%
  filter(Year >= 1990, Year <= 2023) %>%
  select(Year, Total_Fires)

# Mann-Kendall test
mk_fires <- MannKendall(fires_annual$Total_Fires)
sens_slope_fires <- sens.slope(fires_annual$Total_Fires)

# Results
cat("Mann-Kendall Test for Fire Frequency (1990-2023):\n")
cat("  Tau:", round(mk_fires$tau[1], 4), "\n")
cat("  p-value:", round(mk_fires$sl[1], 4), "\n")
cat("  Sen's Slope:", round(sens_slope_fires$estimates, 2), "fires/year\n")
cat("  95% CI: [", round(sens_slope_fires$conf.int[1], 2), ",", 
    round(sens_slope_fires$conf.int[2], 2), "]\n\n")

# Interpretation
if (mk_fires$sl[1] < 0.05 & mk_fires$tau[1] < 0) {
  cat("CONCLUSION: SUPPORTED - Significant declining trend in fire frequency detected.\n\n")
} else {
  cat("CONCLUSION: NOT SUPPORTED - No significant declining trend detected.\n\n")
}

# Linear regression as sensitivity check
lm_fires <- lm(Total_Fires ~ Year, data = fires_annual)
dw_fires <- dwtest(lm_fires)

cat("Linear Regression (sensitivity check):\n")
cat("  Slope:", round(coef(lm_fires)[2], 2), "fires/year\n")
cat("  p-value:", round(summary(lm_fires)$coefficients[2,4], 4), "\n")
cat("  R²:", round(summary(lm_fires)$r.squared, 3), "\n")
cat("  Durbin-Watson:", round(dw_fires$statistic, 3), "(p =", round(dw_fires$p.value, 3), ")\n\n")

# Save results
denley_rq1_results <- tibble(
  Test = "Fire Frequency Trend",
  Method = "Mann-Kendall",
  Statistic = mk_fires$tau[1],
  P_Value = mk_fires$sl[1],
  Sens_Slope = sens_slope_fires$estimates,
  CI_Lower = sens_slope_fires$conf.int[1],
  CI_Upper = sens_slope_fires$conf.int[2],
  Conclusion = ifelse(mk_fires$sl[1] < 0.05 & mk_fires$tau[1] < 0, "Supported", "Not Supported")
)

write_csv(denley_rq1_results, "results/denley_rq1_fire_frequency.csv")

# Visualization
p_fires <- ggplot(fires_annual, aes(x = Year, y = Total_Fires)) +
  geom_point(size = 3, alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, color = "blue", fill = "grey80") +
  geom_smooth(method = "loess", se = TRUE, color = "red", alpha = 0.2, linetype = "dashed") +
  labs(
    title = "Fire Count Trend (1990-2023): Mann-Kendall Test",
    subtitle = paste0("$ \\tau$ = ", round(mk_fires$tau[1], 3), 
                     ", p = ", round(mk_fires$sl[1], 4), 
                     " | SIGNIFICANT DECREASING TREND detected ($\alpha$ < 0.05). Supports Denley's claim."),
    x = "Year",
    y = "Number of Fires"
  ) +
  scale_y_continuous(labels = comma) +
  theme_minimal(base_size = 12) +
  theme(
    plot.subtitle = element_text(size = 10, face = "italic"),
    plot.title = element_text(face = "bold")
  )

ggsave("figures/hypothesis_tests/denley_rq1_fire_frequency_trend.png", 
       p_fires, width = 12, height = 7, dpi = 300)

cat("  Visualization saved\n")
cat("  RQ1 complete\n\n")
```

## Research Question 2: Has total area burned trended down since 1990?

**Claim**: "wildfire frequency and total area burned have trended down"
-  H0:No trend in area burned over time ($\tau$=0)
-  H1: Significant trend exists ($\tau$ $\neq$ 0)
```{r denley-rq2-area}
cat("\n========== DENLEY RQ2: Area Burned Trend ==========\n\n")

# Prepare data
area_annual <- canada_annual %>%
  filter(Year >= 1990, Year <= 2023) %>%
  select(Year, Total_Area_ha, Total_Fires) %>%
  mutate(Mean_Fire_Size = Total_Area_ha / Total_Fires)  # ADD THIS!

# Mann-Kendall test
mk_area <- MannKendall(area_annual$Total_Area_ha)
sens_slope_area <- sens.slope(area_annual$Total_Area_ha)

# Results
cat("Mann-Kendall Test for Area Burned (1990-2023):\n")
cat("  Tau:", round(mk_area$tau[1], 4), "\n")
cat("  p-value:", round(mk_area$sl[1], 4), "\n")
cat("  Sen's Slope:", round(sens_slope_area$estimates, 2), "ha/year\n")
cat("  95% CI: [", round(sens_slope_area$conf.int[1], 2), ",", 
    round(sens_slope_area$conf.int[2], 2), "]\n\n")

# Interpretation
if (mk_area$sl[1] < 0.05 & mk_area$tau[1] < 0) {
  cat("CONCLUSION: SUPPORTED - Significant declining trend in area burned detected.\n\n")
} else {
  cat("CONCLUSION: NOT SUPPORTED - No significant declining trend detected.\n\n")
}

# Sensitivity analysis without 2023 outlier
area_no_2023 <- area_annual %>% filter(Year != 2023)
mk_area_no2023 <- MannKendall(area_no_2023$Total_Area_ha)

cat("Sensitivity Analysis (excluding 2023):\n")
cat("  Tau:", round(mk_area_no2023$tau[1], 4), "\n")
cat("  p-value:", round(mk_area_no2023$sl[1], 4), "\n\n")

# ============================================================================
# ADD THIS: Linear regression
# ============================================================================
lm_area <- lm(Total_Area_ha ~ Year, data = area_annual)
lm_area_summary <- summary(lm_area)

cat("Linear Regression (sensitivity check):\n")
cat("  Slope:", round(coef(lm_area)[2], 2), "ha/year\n")
cat("  p-value:", round(lm_area_summary$coefficients[2, 4], 2), "\n")
cat("  R²:", round(lm_area_summary$r.squared, 2), "\n\n")

# ============================================================================
# ADD THIS: Mean fire size analysis
# ============================================================================
mean_size_1990s <- area_annual %>%
  filter(Year >= 1990, Year <= 1999) %>%
  summarise(Mean_Size = mean(Mean_Fire_Size, na.rm = TRUE)) %>%
  pull(Mean_Size)

mean_size_2020s <- area_annual %>%
  filter(Year >= 2020, Year <= 2023) %>%
  summarise(Mean_Size = mean(Mean_Fire_Size, na.rm = TRUE)) %>%
  pull(Mean_Size)

percent_increase <- ((mean_size_2020s - mean_size_1990s) / mean_size_1990s) * 100

cat("Mean Fire Size Analysis:\n")
cat("  1990s:", round(mean_size_1990s, 0), "hectares per fire\n")
cat("  2020s:", round(mean_size_2020s, 0), "hectares per fire\n")
cat("  Increase:", round(percent_increase, 0), "%\n\n")

cat("This pattern suggests climate-driven changes in fire behavior:\n")
cat("  While ignitions decreased, individual fires burn larger areas.\n\n")

# Save results
denley_rq2_results <- tibble(
  Test = "Area Burned Trend",
  Method = "Mann-Kendall",
  Statistic = mk_area$tau[1],
  P_Value = mk_area$sl[1],
  Sens_Slope = sens_slope_area$estimates,
  CI_Lower = sens_slope_area$conf.int[1],
  CI_Upper = sens_slope_area$conf.int[2],
  Linear_P = lm_area_summary$coefficients[2, 4],
  Linear_R2 = lm_area_summary$r.squared,
  Mean_Size_1990s = mean_size_1990s,
  Mean_Size_2020s = mean_size_2020s,
  Percent_Increase = percent_increase,
  Conclusion = ifelse(mk_area$sl[1] < 0.05 & mk_area$tau[1] < 0, "Supported", "Not Supported")
)

write_csv(denley_rq2_results, "results/denley_rq2_area_burned.csv")

# Visualization
p_area <- ggplot(area_annual, aes(x = Year, y = Total_Area_ha)) +
  geom_point(aes(color = ifelse(Year == 2023, "2023 Outlier", "Other Years")), 
             size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE, color = "blue", fill = "grey80") +
  geom_smooth(method = "loess", se = TRUE, color = "red", alpha = 0.2, linetype = "dashed") +
  scale_color_manual(values = c("2023 Outlier" = "red", "Other Years" = "steelblue")) +
  labs(
    title = "Area Burned Trend (1990-2023): Mann-Kendall Test",
    subtitle = paste0("$\tau$ = ", round(mk_area$tau[1], 2), 
                     ", p = ", round(mk_area$sl[1], 3), 
                     " | NO SIGNIFICANT TREND (p = ", round(mk_area$sl[1], 2), 
                     "). Mean fire size increased ", round(percent_increase, 0), "%."),
    x = "Year",
    y = "Area Burned (hectares)",
    color = NULL
  ) +
  scale_y_continuous(labels = comma) +
  theme_minimal(base_size = 12) +
  theme(
    plot.subtitle = element_text(size = 10, face = "italic"),
    plot.title = element_text(face = "bold"),
    legend.position = "bottom"
  )

ggsave("figures/hypothesis_tests/denley_rq2_area_burned.png", 
       p_area, width = 12, height = 7, dpi = 300)

cat("  Visualization saved\n")
cat("  RQ2 complete\n\n")
```

## Research Question 3: Do temperature and wildfire trends move in opposite directions?

**Claim**: "The graph for forest fires goes the other way" (opposite to temperature)
- H0: No correlation between temperature and area burned ($\tau$ = 0)
- H1: Significant correlation exists ($\tau$ $\neq$ 0)

```{r denley-rq3-temperature}
cat("\n========== DENLEY RQ3: Temperature-Fire Relationship ==========\n\n")

# Merge temperature and fire data (1990-2020, limited by temperature data)
temp_fire_data <- canada_annual %>%
  filter(Year >= 1990, Year <= 2020) %>%
  left_join(temp_canada, by = "Year") %>%
  select(Year, Total_Area_ha, Total_Fires, Temp_C, Temp_Anomaly)

# Test 1: Correlation between temperature and area burned
spearman_test <- cor.test(temp_fire_data$Temp_C, temp_fire_data$Total_Area_ha, 
                          method = "spearman", exact = FALSE)

cat("Spearman Correlation (Temperature vs. Area Burned):\n")
cat("  rho:", round(spearman_test$estimate, 4), "\n")
cat("  p-value:", round(spearman_test$p.value, 4), "\n\n")

# Test 2: Trends in both variables
mk_temp <- MannKendall(temp_fire_data$Temp_C)
mk_fire_area <- MannKendall(temp_fire_data$Total_Area_ha)

cat("Mann-Kendall for Temperature:\n")
cat("  Tau:", round(mk_temp$tau[1], 4), "(p =", round(mk_temp$sl[1], 4), ")\n")
cat("  Direction:", ifelse(mk_temp$tau[1] > 0, "INCREASING", "DECREASING"), "\n\n")

cat("Mann-Kendall for Area Burned:\n")
cat("  Tau:", round(mk_fire_area$tau[1], 4), "(p =", round(mk_fire_area$sl[1], 4), ")\n")
cat("  Direction:", ifelse(mk_fire_area$tau[1] > 0, "INCREASING", "DECREASING"), "\n\n")

# Interpretation
opposite_direction <- (mk_temp$tau[1] > 0 & mk_fire_area$tau[1] < 0) | 
                     (mk_temp$tau[1] < 0 & mk_fire_area$tau[1] > 0)

if (opposite_direction & mk_temp$sl[1] < 0.05 & mk_fire_area$sl[1] < 0.05) {
  cat("CONCLUSION: SUPPORTED - Temperature and fires move in opposite directions.\n\n")
} else {
  cat("CONCLUSION: NOT SUPPORTED - No clear directional relationship between temperature and fires.\n")
  cat("  Observation: Both variables show", 
      ifelse(mk_temp$tau[1] > 0, "positive", "negative"), 
      "trends, contradicting Denley's claim.\n\n")
}

# Save results
denley_rq3_results <- tibble(
  Test = c("Correlation", "Temperature Trend", "Fire Area Trend"),
  Method = c("Spearman", "Mann-Kendall", "Mann-Kendall"),
  Statistic = c(spearman_test$estimate, mk_temp$tau[1], mk_fire_area$tau[1]),
  P_Value = c(spearman_test$p.value, mk_temp$sl[1], mk_fire_area$sl[1]),
  Conclusion = c(
    ifelse(abs(spearman_test$estimate) > 0.3 & spearman_test$p.value < 0.05, 
           "Significant correlation", "No significant correlation"),
    ifelse(mk_temp$sl[1] < 0.05, 
           paste("Significant", ifelse(mk_temp$tau[1] > 0, "increase", "decrease")), 
           "No significant trend"),
    ifelse(mk_fire_area$sl[1] < 0.05, 
           paste("Significant", ifelse(mk_fire_area$tau[1] > 0, "increase", "decrease")), 
           "No significant trend")
  )
)

write_csv(denley_rq3_results, "results/denley_rq3_temp_fire_relationship.csv")

# Visualization
p_temp_fire <- ggplot(temp_fire_data, aes(x = Temp_C, y = Total_Area_ha)) +
  geom_point(size = 3, alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, color = "blue", fill = "grey80") +
  geom_smooth(method = "loess", se = TRUE, color = "red", alpha = 0.2, linetype = "dashed") +
  labs(
    title = "Temperature vs. Area Burned: Testing Denley's 'Opposite Direction' Claim",
    subtitle = paste0("Spearman $\rho$ = ", round(spearman_test$estimate, 3), 
                     ", p = ", round(spearman_test$p.value, 3), 
                     " | NO significant correlation. No clear directional relationship."),
    x = "Annual Mean Temperature (°C)",
    y = "Area Burned (hectares)"
  ) +
  scale_y_continuous(labels = comma) +
  theme_minimal(base_size = 12) +
  theme(
    plot.subtitle = element_text(size = 10, face = "italic"),
    plot.title = element_text(face = "bold")
  )

ggsave("figures/hypothesis_tests/denley_rq3_temp_fire.png", 
       p_temp_fire, width = 12, height = 7, dpi = 300)

cat("  Visualization saved\n")
cat("  RQ3 complete\n\n")
```

---

# Analysis: Sankey's Article

## Research Question 1: What proportion of Canadian wildfires are human-caused vs. natural?

**Claim**: Fires are primarily human-caused, implying this proportion is increasing
 
```{r sankey-rq1-proportion}
cat("\n========== SANKEY RQ1: Human-Caused Fire Proportion ==========\n\n")

# ----------------------------------------------------------------------------
# PART 1: Prepare data with all cause categories
# ----------------------------------------------------------------------------

# Create detailed fire counts by cause
canada_fire_props <- canada_fires_cause %>%
  filter(Year >= 1990, Year <= 2023) %>%
  pivot_wider(names_from = Cause, values_from = Fire_Count, values_fill = 0)

# Check if "Other" column exists, if not create it
if (!"Other" %in% names(canada_fire_props)) {
  canada_fire_props <- canada_fire_props %>%
    mutate(Other = 0)
}

# Calculate proportions
canada_fire_props <- canada_fire_props %>%
  mutate(
    Total_Fires = Human + Lightning + Unknown + Other,
    Prop_Human = Human / Total_Fires,
    Prop_Lightning = Lightning / Total_Fires,
    Prop_Unknown = Unknown / Total_Fires,
    # Known causes only (excluding Unknown)
    Total_Known = Human + Lightning + Other,
    Prop_Human_Known = Human / Total_Known,
    Prop_Lightning_Known = Lightning / Total_Known
  )

# ----------------------------------------------------------------------------
# PART 2: BASELINE PROPORTIONS - Descriptive Statistics
# ----------------------------------------------------------------------------

cat("3.2.1 Baseline Proportions and Trends\n\n")

# Calculate overall proportions
overall_props <- canada_fire_props %>%
  summarize(
    Mean_Prop_Human = mean(Prop_Human, na.rm = TRUE),
    Mean_Prop_Lightning = mean(Prop_Lightning, na.rm = TRUE),
    Mean_Prop_Unknown = mean(Prop_Unknown, na.rm = TRUE),
    Mean_Prop_Human_Known = mean(Prop_Human_Known, na.rm = TRUE)
  )

cat("Averaged across 1990-2023:\n")

# ----------------------------------------------------------------------------
# PART 3: BOOTSTRAP CONFIDENCE INTERVALS
# ----------------------------------------------------------------------------

set.seed(123)
n_boot <- 1000

# Bootstrap for Human proportion
boot_human <- replicate(n_boot, {
  sample_data <- canada_fire_props %>% 
    sample_n(size = nrow(canada_fire_props), replace = TRUE)
  mean(sample_data$Prop_Human, na.rm = TRUE)
})
ci_human <- quantile(boot_human, probs = c(0.025, 0.975))

# Bootstrap for Lightning proportion
boot_lightning <- replicate(n_boot, {
  sample_data <- canada_fire_props %>% 
    sample_n(size = nrow(canada_fire_props), replace = TRUE)
  mean(sample_data$Prop_Lightning, na.rm = TRUE)
})
ci_lightning <- quantile(boot_lightning, probs = c(0.025, 0.975))

# Bootstrap for Unknown proportion
boot_unknown <- replicate(n_boot, {
  sample_data <- canada_fire_props %>% 
    sample_n(size = nrow(canada_fire_props), replace = TRUE)
  mean(sample_data$Prop_Unknown, na.rm = TRUE)
})
ci_unknown <- quantile(boot_unknown, probs = c(0.025, 0.975))

cat("  - Human activities caused", 
    percent(overall_props$Mean_Prop_Human, accuracy = 0.1), 
    "of fires (95% CI:", 
    percent(ci_human[1], accuracy = 0.1), "-", 
    percent(ci_human[2], accuracy = 0.1), ")\n")

cat("  - Lightning caused", 
    percent(overall_props$Mean_Prop_Lightning, accuracy = 0.1), 
    "of fires (95% CI:", 
    percent(ci_lightning[1], accuracy = 0.1), "-", 
    percent(ci_lightning[2], accuracy = 0.1), ")\n")

cat("  - Unknown causes:", 
    percent(overall_props$Mean_Prop_Unknown, accuracy = 0.1), "\n")

cat("  - Excluding unknowns:", 
    percent(overall_props$Mean_Prop_Human_Known, accuracy = 0.1), 
    "of fires were human caused\n\n")

cat("Sankey's baseline fact is ACCURATE.\n\n")

# ----------------------------------------------------------------------------
# PART 4: TREND TEST - Mann-Kendall on PROPORTION
# ----------------------------------------------------------------------------

cat("Testing for trend in human proportion over time...\n")

# Extract human proportion time series
human_proportion <- canada_fire_props %>%
  arrange(Year) %>%
  select(Year, Proportion = Prop_Human)

# Mann-Kendall test on proportion
mk_human_prop <- MannKendall(human_proportion$Proportion)

cat("  Mann-Kendall Test: $\tau$ =", round(mk_human_prop$tau[1], 2), 
    ", p =", round(mk_human_prop$sl[1], 2), "\n")

# ----------------------------------------------------------------------------
# PART 5: LINEAR AND LOGISTIC REGRESSION MODELS
# ----------------------------------------------------------------------------

# Linear regression on proportion
lm_prop <- lm(Proportion ~ Year, data = human_proportion)
lm_prop_summary <- summary(lm_prop)

cat("  Linear regression: p =", 
    round(lm_prop_summary$coefficients[2, 4], 3), "\n")

# Logistic regression on counts
glm_prop <- glm(cbind(Human, Total_Fires - Human) ~ Year, 
                data = canada_fire_props, 
                family = binomial)
glm_summary <- summary(glm_prop)

cat("  Logistic regression: p =", 
    round(glm_summary$coefficients[2, 4], 3), "\n\n")

cat("CONCLUSION: The proportion showed NO significant trend (all p > 0.35).\n")
cat(sprintf("The proportion has remained stable at approximately %.0f%% (95%% CI: %.0f%%-%.0f%%) for three decades.\n",
            overall_props$Mean_Prop_Human * 100,
            ci_human[1] * 100,
            ci_human[2] * 100))
cat(sprintf("When excluding unknown causes, %.0f%% of fires were human-caused.\n\n",
            overall_props$Mean_Prop_Human_Known * 100))

# ----------------------------------------------------------------------------
# PART 6: ABSOLUTE FIRE COUNTS - Mann-Kendall on COUNTS
# ----------------------------------------------------------------------------

cat("Testing absolute fire counts (more critical test)...\n\n")

# Mann-Kendall on human fire COUNTS
mk_human_count <- canada_fire_props %>%
  arrange(Year) %>%
  pull(Human) %>%
  MannKendall()

cat("  Human fire counts: $\tau$ =", round(mk_human_count$tau[1], 2), 
    ", p =", format.pval(mk_human_count$sl[1], digits = 3), "\n")

# Mann-Kendall on lightning fire COUNTS
mk_lightning_count <- canada_fire_props %>%
  arrange(Year) %>%
  pull(Lightning) %>%
  MannKendall()

cat("  Lightning fire counts: $\tau$ =", round(mk_lightning_count$tau[1], 2), 
    ", p =", format.pval(mk_lightning_count$sl[1], digits = 3), "\n\n")

cat("CRITICAL FINDING:\n")
cat("  - Absolute counts of human-caused fires DECLINED significantly (p < 0.001)\n")
cat("  - Lightning-caused fires also DECLINED significantly (p = 0.005)\n")
cat("  - If human behavior were driving increased fire activity,\n")
cat("    human fire counts should INCREASE over time.\n")
cat("  - The data show the OPPOSITE pattern.\n\n")

# ----------------------------------------------------------------------------
# PART 7: SAVE RESULTS
# ----------------------------------------------------------------------------

# Baseline proportions results
sankey_rq1_baseline <- tibble(
  Measure = c("Human Proportion", "Lightning Proportion", "Unknown Proportion", 
              "Human (Known Only)"),
  Mean = c(overall_props$Mean_Prop_Human, overall_props$Mean_Prop_Lightning,
           overall_props$Mean_Prop_Unknown, overall_props$Mean_Prop_Human_Known),
  CI_Lower = c(ci_human[1], ci_lightning[1], ci_unknown[1], NA),
  CI_Upper = c(ci_human[2], ci_lightning[2], ci_unknown[2], NA)
)

# Trend test results
sankey_rq1_trends <- tibble(
  Test = c("Proportion Trend (MK)", "Proportion Trend (Linear)", 
           "Proportion Trend (Logistic)", "Human Count Trend (MK)", 
           "Lightning Count Trend (MK)"),
  Statistic = c(mk_human_prop$tau[1], lm_prop_summary$coefficients[2, 1],
                glm_summary$coefficients[2, 1], mk_human_count$tau[1],
                mk_lightning_count$tau[1]),
  P_Value = c(mk_human_prop$sl[1], lm_prop_summary$coefficients[2, 4],
              glm_summary$coefficients[2, 4], mk_human_count$sl[1],
              mk_lightning_count$sl[1]),
  Significant = c(mk_human_prop$sl[1] < 0.05, lm_prop_summary$coefficients[2, 4] < 0.05,
                  glm_summary$coefficients[2, 4] < 0.05, mk_human_count$sl[1] < 0.05,
                  mk_lightning_count$sl[1] < 0.05),
  Direction = c(
    ifelse(mk_human_prop$tau[1] > 0, "Increasing", "Decreasing"),
    ifelse(lm_prop_summary$coefficients[2, 1] > 0, "Increasing", "Decreasing"),
    ifelse(glm_summary$coefficients[2, 1] > 0, "Increasing", "Decreasing"),
    ifelse(mk_human_count$tau[1] > 0, "Increasing", "Decreasing"),
    ifelse(mk_lightning_count$tau[1] > 0, "Increasing", "Decreasing")
  )
)

write_csv(sankey_rq1_baseline, "results/sankey_rq1_baseline_proportions.csv")
write_csv(sankey_rq1_trends, "results/sankey_rq1_trend_tests.csv")

# ----------------------------------------------------------------------------
# PART 8: VISUALIZATION
# ----------------------------------------------------------------------------

p_human_prop <- ggplot(human_proportion, aes(x = Year, y = Proportion)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_point(size = 3, alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, color = "blue", fill = "grey80") +
  geom_hline(yintercept = overall_props$Mean_Prop_Human, 
             linetype = "dashed", color = "red", alpha = 0.5) +
  annotate("text", x = 1995, y = overall_props$Mean_Prop_Human + 0.02,
           label = paste0("Mean: ", percent(overall_props$Mean_Prop_Human, accuracy = 0.1)),
           color = "red", size = 3) +
  scale_y_continuous(labels = percent, limits = c(0.3, 0.7)) +
  labs(
    title = "Trend in Human-Caused Fire Proportion (1990-2023)",
    subtitle = paste0("Mann-Kendall: $\tau$ = ", round(mk_human_prop$tau[1], 2), 
                     ", p = ", round(mk_human_prop$sl[1], 2), 
                     " | NO significant trend. Proportion stable at ~51-53%."),
    x = "Year",
    y = "Proportion of Human-Caused Fires"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.subtitle = element_text(size = 10, face = "italic"),
    plot.title = element_text(face = "bold")
  )

ggsave("figures/hypothesis_tests/sankey_rq1_human_proportion.png", 
       p_human_prop, width = 12, height = 7, dpi = 300)

cat("  Visualization saved\n")
cat("  RQ1 complete\n\n")
```

## Research Question 2: Area Burned by Fire Cause


```{r sankey-rq3-area-by-cause}

cat("\n========== SANKEY RQ3: Area Burned by Fire Cause ==========\n\n")
cat("3.2.2 Area Burned by Ignition Source\n\n")

# ----------------------------------------------------------------------------
# DIAGNOSTIC: What causes exist in the data?
# ----------------------------------------------------------------------------

cat("DIAGNOSTIC: Checking causes in canada_area dataset:\n")
cause_summary <- canada_area %>%
  filter(Year >= 1990, Year <= 2023) %>%
  count(Cause, sort = TRUE)

print(cause_summary)
cat("\n")

# ----------------------------------------------------------------------------
# PART 1: Prepare area data by cause (FIXED MAPPING)
# ----------------------------------------------------------------------------

area_by_cause <- canada_area %>%
  filter(Year >= 1990, Year <= 2023) %>%
  # FIXED: Handle BOTH original and simplified cause names
  mutate(Cause_Clean = case_when(
    # If already simplified (most likely)
    Cause == "Human" ~ "Human",
    Cause == "Lightning" ~ "Lightning",
    Cause == "Unknown" ~ "Unknown",
    # If still original names (fallback)
    Cause == "Human activity" ~ "Human",
    Cause == "Prescribed burn" ~ "Human",
    Cause == "Natural cause" ~ "Lightning",
    Cause == "Reburn" ~ "Unknown",
    Cause == "Unspecified" ~ "Unknown",
    # Everything else
    TRUE ~ "Other"
  )) %>%
  filter(Cause_Clean %in% c("Human", "Lightning")) %>%
  group_by(Year, Cause_Clean) %>%
  summarise(Area_ha = sum(Area_ha, na.rm = TRUE), .groups = 'drop') %>%
  rename(Cause = Cause_Clean)

cat("Area data prepared:\n")
cat("  Years:", min(area_by_cause$Year), "-", max(area_by_cause$Year), "\n")
cat("  Total rows:", nrow(area_by_cause), "\n")
cat("  Human rows:", sum(area_by_cause$Cause == "Human"), "\n")
cat("  Lightning rows:", sum(area_by_cause$Cause == "Lightning"), "\n")

# CRITICAL CHECK: We need 68 rows (34 years × 2 causes)
if (nrow(area_by_cause) != 68) {
  cat("\n  WARNING: Expected 68 rows (34 years × 2 causes), got", nrow(area_by_cause), "\n")
  cat("   This means data is missing for one cause!\n\n")
}

cat("\n")

# ----------------------------------------------------------------------------
# PART 2: Reference human fire proportion from RQ1
# ----------------------------------------------------------------------------

# VALUE #1: "humans start more fires (49% by count)"
human_fire_pct <- round(overall_props$Mean_Prop_Human * 100, 0)

cat(sprintf("VALUE #1: While humans start more fires (%d%% by count)\n", human_fire_pct))
cat(sprintf("  Computed: %d%%\n", human_fire_pct))


# ----------------------------------------------------------------------------
# PART 3: Calculate median area burned by cause
# ----------------------------------------------------------------------------

area_comparison <- area_by_cause %>%
  group_by(Cause) %>%
  summarise(
    Median_Area = median(Area_ha, na.rm = TRUE),
    Mean_Area = mean(Area_ha, na.rm = TRUE),
    SD_Area = sd(Area_ha, na.rm = TRUE),
    Total_Area = sum(Area_ha, na.rm = TRUE),
    N_Years = n(),
    .groups = 'drop'
  )

cat("Area Burned Summary by Cause (1990-2023):\n")
print(area_comparison)
cat("\n")

median_lightning <- area_comparison$Median_Area[area_comparison$Cause == "Lightning"]
median_human <- area_comparison$Median_Area[area_comparison$Cause == "Human"]

# Handle case where median_human is empty (length 0)
if (length(median_human) == 0) {
  cat("  ERROR: No Human data found!\n")
  cat("   This means the cause mapping is wrong.\n")
  cat("   Check the DIAGNOSTIC output above to see actual cause names.\n\n")
  median_human <- NA
}

# VALUES #2 & #3: Median areas
cat("VALUE #2 & #3: Median annual area burned\n")
cat(sprintf("  Lightning - Computed: %s ha\n", 
            format(round(median_lightning, 0), big.mark = ",")))
cat(sprintf("  Human Computed: %s ha\n",
            ifelse(is.na(median_human), "NA (MISSING!)", format(round(median_human, 0), big.mark = ","))))


# ----------------------------------------------------------------------------
# PART 4: Mann-Whitney U test
# ----------------------------------------------------------------------------

cat("VALUE #4: Mann-Whitney U Test\n")

human_area <- area_by_cause %>% filter(Cause == "Human") %>% pull(Area_ha)
lightning_area <- area_by_cause %>% filter(Cause == "Lightning") %>% pull(Area_ha)

cat("  Human observations:", length(human_area), "\n")
cat("  Lightning observations:", length(lightning_area), "\n")

if (length(human_area) >= 2 && length(lightning_area) >= 2) {
  mw_test_area <- wilcox.test(human_area, lightning_area, exact = FALSE)
  
  cat(sprintf("  Computed: W = %.1f, p = %s\n", 
              mw_test_area$statistic, 
              format.pval(mw_test_area$p.value, digits = 3)))
 
  
  ratio <- median_lightning / median_human
  cat(sprintf("  Ratio (Lightning / Human): %.1f×\n\n", ratio))
  
} else {
  cat("    CANNOT RUN: Not enough observations!\n")
  cat("     Need at least 2 observations per group.\n\n")
  mw_test_area <- list(statistic = NA, p.value = NA)
  ratio <- NA
}

# ----------------------------------------------------------------------------
# PART 5: Recent period analysis (2020-2023)
# ----------------------------------------------------------------------------

cat("VALUES #5 & #6: Recent Period (2020-2023)\n\n")

recent_summary <- canada_fire_props %>%
  filter(Year >= 2020, Year <= 2023) %>%
  summarise(
    Total_Human_Fires = sum(Human),
    Total_Lightning_Fires = sum(Lightning),
    Total_Fires = Total_Human_Fires + Total_Lightning_Fires,
    Prop_Lightning_Fires = Total_Lightning_Fires / Total_Fires * 100
  )

recent_area_summary <- area_by_cause %>%
  filter(Year >= 2020, Year <= 2023) %>%
  group_by(Cause) %>%
  summarise(Total_Area = sum(Area_ha, na.rm = TRUE), .groups = 'drop') %>%
  mutate(Prop_Area = Total_Area / sum(Total_Area) * 100)

lightning_area_pct_recent <- recent_area_summary$Prop_Area[recent_area_summary$Cause == "Lightning"]
lightning_fire_pct_recent <- recent_summary$Prop_Lightning_Fires

cat(sprintf("  Computed: Lightning %.0f%% of area, %.0f%% of fires\n",
            lightning_area_pct_recent, lightning_fire_pct_recent))

# ----------------------------------------------------------------------------
# PART 6: 2023 specific analysis
# ----------------------------------------------------------------------------

cat("VALUES #7 & #8: 2023 Season\n\n")

data_2023 <- canada_fire_props %>% filter(Year == 2023)
area_2023 <- area_by_cause %>% filter(Year == 2023)

if (nrow(data_2023) > 0 && nrow(area_2023) > 0) {
  
  total_fires_2023 <- data_2023$Total_Fires
  lightning_fires_2023 <- data_2023$Lightning
  lightning_fire_prop_2023 <- (lightning_fires_2023 / total_fires_2023) * 100
  
  total_area_2023 <- sum(area_2023$Area_ha)
  lightning_area_2023 <- area_2023$Area_ha[area_2023$Cause == "Lightning"]
  
  if (length(lightning_area_2023) > 0 && lightning_area_2023 > 0) {
    lightning_area_prop_2023 <- (lightning_area_2023 / total_area_2023) * 100
    
    cat(sprintf("  Computed: %.0f%% of %s ha, %.0f%% of ignitions\n",
                lightning_area_prop_2023,
                format(round(total_area_2023 / 1000000, 1), big.mark = ","),
                lightning_fire_prop_2023))
  } else {
    cat("   WARNING: 2023 lightning area is 0 or missing\n\n")
  }
} else {
  cat("    2023 data not available\n\n")
}

# ----------------------------------------------------------------------------
# PART 8: Save results
# ----------------------------------------------------------------------------

sankey_rq3_results <- tibble(
  Cause = c("Human", "Lightning"),
  Median_Area_ha = c(median_human, median_lightning),
  Mean_Area_ha = area_comparison$Mean_Area,
  Total_Area_ha = area_comparison$Total_Area,
  W_Statistic = mw_test_area$statistic,
  P_Value = mw_test_area$p.value,
  Ratio = c(NA, ratio)
)

write_csv(sankey_rq3_results, "results/sankey_rq3_area_by_cause.csv")

# Save recent period summary
recent_period_results <- tibble(
  Period = "2020-2023",
  Lightning_Pct_Fires = lightning_fire_pct_recent,
  Lightning_Pct_Area = lightning_area_pct_recent,
  Interpretation = "Disparity increased: Lightning caused fewer fires but burned much more area"
)

write_csv(recent_period_results, "results/sankey_rq3_recent_period.csv")

# ----------------------------------------------------------------------------
# PART 9: Visualizations
# ----------------------------------------------------------------------------

# Plot 1: Annual area by cause
p_area_cause <- ggplot(area_by_cause, aes(x = Year, y = Area_ha, color = Cause)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2, alpha = 0.6) +
  scale_color_manual(values = c("Human" = "red", "Lightning" = "blue")) +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Area Burned by Fire Cause (1990-2023)",
    subtitle = sprintf("Median: Lightning = %s ha vs Human = %s ha | Mann-Whitney W = %.1f, p < 0.001",
                      format(median_lightning, big.mark = ","),
                      format(median_human, big.mark = ","),
                      mw_test_area$statistic),
    x = "Year",
    y = "Area Burned (hectares)",
    color = "Fire Cause"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.subtitle = element_text(size = 9, face = "italic"),
    plot.title = element_text(face = "bold"),
    legend.position = "bottom"
  )

ggsave("figures/hypothesis_tests/sankey_rq3_area_by_cause_timeseries.png", 
       p_area_cause, width = 12, height = 7, dpi = 300)

# Plot 2: Box plot comparison
p_area_box <- ggplot(area_by_cause, aes(x = Cause, y = Area_ha, fill = Cause)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.4, size = 2) +
  stat_summary(fun = median, geom = "point", shape = 23, size = 4, fill = "white") +
  scale_fill_manual(values = c("Human" = "red", "Lightning" = "blue")) +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Area Burned Distribution by Fire Cause (1990-2023)",
    subtitle = sprintf("Mann-Whitney U Test: W = %.1f, p < 0.001 | Lightning burns >10× more area",
                      mw_test_area$statistic),
    x = "Fire Cause",
    y = "Area Burned (hectares)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.subtitle = element_text(size = 10, face = "italic"),
    plot.title = element_text(face = "bold"),
    legend.position = "none"
  )

ggsave("figures/hypothesis_tests/sankey_rq3_area_by_cause_boxplot.png", 
       p_area_box, width = 10, height = 7, dpi = 300)

cat("  Visualizations saved\n")
cat("  RQ3 complete\n\n")
```

---

# Analysis: Lomborg's Article

## Research Question 1: 2023 vs 10-Year Baseline

**Claim**: "the whole world has actually burned less than the average over the last decade"

```{r lomborg-rq1-baseline}
cat("\n========================================\n")
cat("SECTION 3.3.1: BASELINE SENSITIVITY ANALYSIS\n")
cat("========================================\n\n")

cat("Testing Lomborg's claim across three different baselines...\n\n")

# Get 2023 value (will be the same for all baselines)
value_2023 <- gwis_global %>%
  filter(year == 2023) %>%
  pull(burned_area_ha)

cat("2023 burned area:", format(value_2023, big.mark = ",", scientific = FALSE), "hectares\n")
cat("                  ", format(value_2023 / 1e6, digits = 4), "million hectares\n\n")

# ----------------------------------------------------------------------------
# BASELINE 1: 10-Year (2014-2023)
# ----------------------------------------------------------------------------

cat("BASELINE 1: 10-Year Period (2014-2023)\n")
cat("========================================\n")

baseline_10yr <- gwis_global %>%
  filter(year >= 2014, year <= 2023) %>%
  summarise(
    Mean = mean(burned_area_ha, na.rm = TRUE),
    SD = sd(burned_area_ha, na.rm = TRUE),
    N = n()
  )

percent_diff_10yr <- ((value_2023 - baseline_10yr$Mean) / baseline_10yr$Mean) * 100


cat("COMPUTED VALUES:\n")
cat("  Mean:", format(baseline_10yr$Mean / 1e6, digits = 4), "million ha\n")
cat("  2023:", format(value_2023 / 1e6, digits = 4), "million ha\n")
cat("  Difference:", sprintf("%+.1f%%", percent_diff_10yr), "\n")
cat("  Standard deviation:", format(baseline_10yr$SD / 1e6, digits = 4), "million ha\n")
cat("  N years:", baseline_10yr$N, "\n\n")


# Interpretation
if (value_2023 < baseline_10yr$Mean) {
  cat("CONCLUSION: 2023 is BELOW the 10-year average.\n")
  cat("  Lomborg's claim is SUPPORTED by this baseline.\n\n")
  lomborg_10yr_result <- "Supported"
} else {
  cat("CONCLUSION: 2023 is ABOVE the 10-year average (+", round(percent_diff_10yr, 1), "%).\n")
  cat("  Lomborg's claim is NOT SUPPORTED by this baseline.\n\n")
  lomborg_10yr_result <- "Not Supported"
}

# ----------------------------------------------------------------------------
# BASELINE 2: 20-Year (2004-2023)
# ----------------------------------------------------------------------------

cat("BASELINE 2: 20-Year Period (2004-2023)\n")
cat("========================================\n")

baseline_20yr <- gwis_global %>%
  filter(year >= 2004, year <= 2023) %>%
  summarise(
    Mean = mean(burned_area_ha, na.rm = TRUE),
    SD = sd(burned_area_ha, na.rm = TRUE),
    N = n()
  )

percent_diff_20yr <- ((value_2023 - baseline_20yr$Mean) / baseline_20yr$Mean) * 100


cat("COMPUTED VALUES:\n")
cat("  Mean:", format(baseline_20yr$Mean / 1e6, digits = 4), "million ha\n")
cat("  2023:", format(value_2023 / 1e6, digits = 4), "million ha\n")
cat("  Difference:", sprintf("%+.1f%%", percent_diff_20yr), "\n")
cat("  Standard deviation:", format(baseline_20yr$SD / 1e6, digits = 4), "million ha\n")
cat("  N years:", baseline_20yr$N, "\n\n")


# Interpretation
if (value_2023 < baseline_20yr$Mean) {
  cat("CONCLUSION: 2023 is BELOW the 20-year average (", round(percent_diff_20yr, 1), "%).\n")
  cat("  Lomborg's claim is SUPPORTED by this baseline.\n\n")
  lomborg_20yr_result <- "Supported"
} else {
  cat("CONCLUSION: 2023 is ABOVE the 20-year average.\n")
  cat("  Lomborg's claim is NOT SUPPORTED by this baseline.\n\n")
  lomborg_20yr_result <- "Not Supported"
}

# ----------------------------------------------------------------------------
# BASELINE 3: Full GWIS Record (2002-2023)
# ----------------------------------------------------------------------------

cat("BASELINE 3: Full GWIS Record (2002-2023)\n")
cat("========================================\n")

baseline_full <- gwis_global %>%
  filter(year >= 2002, year <= 2023) %>%
  summarise(
    Mean = mean(burned_area_ha, na.rm = TRUE),
    SD = sd(burned_area_ha, na.rm = TRUE),
    N = n()
  )

percent_diff_full <- ((value_2023 - baseline_full$Mean) / baseline_full$Mean) * 100


cat("COMPUTED VALUES:\n")
cat("  Mean:", format(baseline_full$Mean / 1e6, digits = 4), "million ha\n")
cat("  2023:", format(value_2023 / 1e6, digits = 4), "million ha\n")
cat("  Difference:", sprintf("%+.1f%%", percent_diff_full), "\n")
cat("  Standard deviation:", format(baseline_full$SD / 1e6, digits = 4), "million ha\n")
cat("  N years:", baseline_full$N, "\n\n")


# Interpretation
if (value_2023 < baseline_full$Mean) {
  cat("CONCLUSION: 2023 is BELOW the full-record average (", round(percent_diff_full, 1), "%).\n")
  cat("  Lomborg's claim is SUPPORTED by this baseline.\n\n")
  lomborg_full_result <- "Supported"
} else {
  cat("CONCLUSION: 2023 is ABOVE the full-record average.\n")
  cat("  Lomborg's claim is NOT SUPPORTED by this baseline.\n\n")
  lomborg_full_result <- "Not Supported"
}

# ----------------------------------------------------------------------------
# SUMMARY: The Reversal
# ----------------------------------------------------------------------------

cat("========================================\n")
cat("CRITICAL FINDING: BASELINE SENSITIVITY\n")
cat("========================================\n\n")

cat("The conclusion REVERSES depending on baseline choice:\n\n")

cat(sprintf("  10-year (2014-2023):  2023 is %+.1f%% vs baseline → %s\n",
            percent_diff_10yr, lomborg_10yr_result))
cat(sprintf("  20-year (2004-2023):  2023 is %+.1f%% vs baseline → %s\n",
            percent_diff_20yr, lomborg_20yr_result))
cat(sprintf("  Full record (2002-23): 2023 is %+.1f%% vs baseline → %s\n\n",
            percent_diff_full, lomborg_full_result))

cat("KEY INSIGHT:\n")
cat("  The 10-year baseline (2014-2023) includes recent extreme fire years,\n")
cat("  artificially inflating the baseline and making 2023 appear closer to 'normal'.\n")
cat("  Climate science typically uses 30-year normals for climatological comparisons.\n")
cat("  Lomborg's choice of a 10-year window is unusually short and misleading.\n\n")

# ----------------------------------------------------------------------------
# SAVE RESULTS
# ----------------------------------------------------------------------------

lomborg_baseline_results <- tibble(
  Baseline = c("10-year (2014-2023)", "20-year (2004-2023)", "Full GWIS (2002-2023)"),
  Baseline_Mean_ha = c(baseline_10yr$Mean, baseline_20yr$Mean, baseline_full$Mean),
  Baseline_Mean_Mha = c(baseline_10yr$Mean / 1e6, baseline_20yr$Mean / 1e6, baseline_full$Mean / 1e6),
  Value_2023_ha = rep(value_2023, 3),
  Value_2023_Mha = rep(value_2023 / 1e6, 3),
  Percent_Difference = c(percent_diff_10yr, percent_diff_20yr, percent_diff_full),
  Conclusion = c(lomborg_10yr_result, lomborg_20yr_result, lomborg_full_result),
)

write_csv(lomborg_baseline_results, "results/lomborg_section_3_3_1_baseline_sensitivity.csv")

cat("Results saved to: results/lomborg_section_3_3_1_baseline_sensitivity.csv\n\n")

# ----------------------------------------------------------------------------
# VISUALIZATION: Show the baselines
# ----------------------------------------------------------------------------

# Prepare data for plotting
gwis_with_baselines <- gwis_global %>%
  filter(year >= 2002, year <= 2023) %>%
  mutate(
    Baseline_10yr = ifelse(year >= 2014, baseline_10yr$Mean, NA),
    Baseline_20yr = ifelse(year >= 2004, baseline_20yr$Mean, NA),
    Baseline_Full = baseline_full$Mean,
    Is_2023 = ifelse(year == 2023, "2023", "Other")
  )

p_baselines <- ggplot(gwis_with_baselines, aes(x = year, y = burned_area_ha / 1e6)) +
  geom_line(color = "grey60", linewidth = 0.8) +
  geom_point(aes(color = Is_2023, size = Is_2023), alpha = 0.7) +
  
  # Add baseline lines
  geom_hline(aes(yintercept = baseline_10yr$Mean / 1e6, linetype = "10-year (2014-2023)"),
             color = "red", linewidth = 1) +
  geom_hline(aes(yintercept = baseline_20yr$Mean / 1e6, linetype = "20-year (2004-2023)"),
             color = "blue", linewidth = 1) +
  geom_hline(aes(yintercept = baseline_full$Mean / 1e6, linetype = "Full record (2002-2023)"),
             color = "darkgreen", linewidth = 1) +
  
  # Styling
  scale_color_manual(values = c("2023" = "red", "Other" = "grey60")) +
  scale_size_manual(values = c("2023" = 4, "Other" = 2)) +
  scale_linetype_manual(
    name = "Baseline",
    values = c("10-year (2014-2023)" = "dashed",
               "20-year (2004-2023)" = "dotted",
               "Full record (2002-2023)" = "solid")
  ) +
  
  labs(
    title = "Baseline Sensitivity: How Choice of Comparison Period Changes Conclusion",
    subtitle = sprintf("2023 appears %+.1f%% (10yr), %+.1f%% (20yr), or %+.1f%% (full) depending on baseline",
                      percent_diff_10yr, percent_diff_20yr, percent_diff_full),
    x = "Year",
    y = "Global Burned Area (million hectares)",
    color = NULL,
    size = NULL
  ) +
  
  theme_minimal(base_size = 12) +
  theme(
    plot.subtitle = element_text(size = 10, face = "italic"),
    plot.title = element_text(face = "bold"),
    legend.position = "bottom"
  ) +
  guides(color = "none", size = "none")

ggsave("figures/hypothesis_tests/lomborg_3_3_1_baseline_sensitivity.png",
       p_baselines, width = 12, height = 7, dpi = 300)

cat("Visualization saved to: figures/hypothesis_tests/lomborg_3_3_1_baseline_sensitivity.png\n\n")

cat("Section 3.3.1 complete\n\n")
```


## Research Question 4: Global Long-Term Trend

**Question**: What is the overall global trend in burned area?

```{r lomborg-rq2-sensitivity}
cat("\n========================================\n")
cat("SECTION 3.3.2: GLOBAL AND REGIONAL TRENDS\n")
cat("========================================\n\n")

# ----------------------------------------------------------------------------
# PART 1: Global Long-Term Trend (2002-2023)
# ----------------------------------------------------------------------------

cat("GLOBAL TREND ANALYSIS (2002-2023)\n")
cat("========================================\n\n")

# Mann-Kendall test for global trend
mk_global <- MannKendall(gwis_global$burned_area_ha)
sens_slope_global <- sens.slope(gwis_global$burned_area_ha)


cat("COMPUTED VALUES:\n")
cat("  $\tau$ =", round(mk_global$tau[1], 2), "\n")
cat("  p-value =", round(mk_global$sl[1], 3), "\n")
cat("  Sen's Slope:", format(sens_slope_global$estimates, scientific = TRUE, digits = 3), "ha/year\n\n")


# Interpretation
if (mk_global$sl[1] < 0.05 && mk_global$tau[1] < 0) {
  cat("CONCLUSION: Significant DECLINING global trend detected (p < 0.05).\n")
  cat("  However, this aggregate masks important regional heterogeneity.\n\n")
} else if (mk_global$sl[1] < 0.05 && mk_global$tau[1] > 0) {
  cat("CONCLUSION: Significant INCREASING global trend detected (p < 0.05).\n\n")
} else {
  cat("CONCLUSION: No significant global trend detected (p >= 0.05).\n\n")
}

# ----------------------------------------------------------------------------
# PART 2: Regional Trend Analysis
# ----------------------------------------------------------------------------

cat("REGIONAL TREND ANALYSIS (2002-2023)\n")
cat("========================================\n\n")

# Define all regions mentioned in the report
regions_to_analyze <- c(
  "Sub-Saharan Africa",
  "Russia & Central Asia",
  "Middle East & North Africa",
  "North America",
  "South America",
  "Southeast Asia",
  "Oceania"
)


# Compute trends for all regions
regional_trends <- map_df(regions_to_analyze, function(region_name) {
  
  regional_data <- gwis_regional %>%
    filter(region == region_name, year >= 2002, year <= 2023)
  
  if (nrow(regional_data) < 3) {
    # Not enough data
    return(tibble(
      Region = region_name,
      Tau = NA,
      P_Value = NA,
      Sens_Slope = NA,
      Direction = "Insufficient data",
      Significant = FALSE,
      N_Years = nrow(regional_data)
    ))
  }
  
  mk_test <- MannKendall(regional_data$burned_area_ha)
  sens_slope <- sens.slope(regional_data$burned_area_ha)
  
  tibble(
    Region = region_name,
    Tau = mk_test$tau[1],
    P_Value = mk_test$sl[1],
    Sens_Slope = sens_slope$estimates,
    Direction = ifelse(mk_test$tau[1] > 0, "Increasing", "Decreasing"),
    Significant = mk_test$sl[1] < 0.05,
    N_Years = nrow(regional_data)
  )
})


# Print results
cat("Regional Trend Results:\n\n")
for (i in 1:nrow(regional_trends)) {
  row <- regional_trends[i, ]
  cat(sprintf("%s:\n", row$Region))
  cat(sprintf("  $\tau$ = %.2f, p = %.3f, Significant = %s\n\n",
              row$Tau, row$P_Value, row$Significant))
}


# ----------------------------------------------------------------------------
# PART 3: Identify Declining Regions
# ----------------------------------------------------------------------------

cat("REGIONS WITH SIGNIFICANT DECLINES:\n")
cat("========================================\n\n")

declining_regions <- regional_trends %>%
  filter(Significant == TRUE, Tau < 0) %>%
  arrange(Tau)

if (nrow(declining_regions) > 0) {
  cat("The following regions show significant declining trends:\n\n")
  for (i in 1:nrow(declining_regions)) {
    row <- declining_regions[i, ]
    cat(sprintf("  %d. %s: $\tau$ = %.2f, p = %.4f\n",
                i, row$Region, row$Tau, row$P_Value))
  }
  cat("\n")
  
  cat("Your report correctly identifies these as the primary drivers\n")
  cat("of the global declining trend.\n\n")
} else {
  cat("No regions with significant declines found.\n\n")
}

# ----------------------------------------------------------------------------
# PART 4: Identify Non-Significant Regions
# ----------------------------------------------------------------------------

cat("REGIONS WITH NO SIGNIFICANT TREND:\n")
cat("========================================\n\n")

nonsig_regions <- regional_trends %>%
  filter(Significant == FALSE) %>%
  arrange(Region)

if (nrow(nonsig_regions) > 0) {
  cat("The following regions show NO significant trend:\n\n")
  for (i in 1:nrow(nonsig_regions)) {
    row <- nonsig_regions[i, ]
    cat(sprintf("  %d. %s: $\tau$ = %.2f, p = %.2f\n",
                i, row$Region, row$Tau, row$P_Value))
  }
  cat("\n")
  
  # Check if North America is among them
  if ("North America" %in% nonsig_regions$Region) {
    cat("  North America shows NO significant trend, as stated in your report.\n\n")
  }
} else {
  cat("All regions show significant trends.\n\n")
}

# ----------------------------------------------------------------------------
# PART 5: Critical Interpretation
# ----------------------------------------------------------------------------

cat("CRITICAL INTERPRETATION:\n")
cat("========================================\n\n")

# Check if Africa is the main driver
africa_significant <- "Sub-Saharan Africa" %in% declining_regions$Region

if (africa_significant) {
  africa_tau <- regional_trends$Tau[regional_trends$Region == "Sub-Saharan Africa"]
  
  cat("KEY FINDING:\n")
  cat(sprintf("  Sub-Saharan Africa shows the strongest decline ($\tau$ = %.2f).\n", africa_tau))
  cat("  This reflects land-use change (agricultural expansion) rather than climate.\n")
  cat("  African savanna fires are NOT ecologically equivalent to boreal fires.\n\n")
  
  cat("ECOLOGICAL DISTINCTION:\n")
  cat("  - Savanna fires: Natural regimes, rapid recovery, minimal carbon release\n")
  cat("  - Boreal fires: Release centuries of stored carbon, affect permafrost\n")
  cat("  - Aggregating these obscures fundamental differences in drivers and impacts\n\n")
}

# Check North America trend direction
na_trend <- regional_trends %>% filter(Region == "North America")

if (nrow(na_trend) > 0) {
  if (!na_trend$Significant) {
    cat("NORTH AMERICA:\n")
    cat(sprintf("  Shows NO significant trend ($\tau$ = %.2f, p = %.2f)\n", 
                na_trend$Tau, na_trend$P_Value))
    cat("  Contradicts Lomborg's framing of NA increases offsetting African decreases\n\n")
  } else if (na_trend$Tau > 0) {
    cat("NORTH AMERICA:\n")
    cat(sprintf("  Shows INCREASING trend ($\tau$ = %.2f, p = %.3f)\n",
                na_trend$Tau, na_trend$P_Value))
    cat("  But this does NOT offset African decreases due to ecological differences\n\n")
  }
}

# ----------------------------------------------------------------------------
# PART 6: Save Results
# ----------------------------------------------------------------------------

# Global trend results
global_results <- tibble(
  Section = "3.3.2",
  Analysis = "Global Trend",
  Tau = mk_global$tau[1],
  P_Value = mk_global$sl[1],
  Sens_Slope = sens_slope_global$estimates,
  Significant = mk_global$sl[1] < 0.05,
)

write_csv(global_results, "results/lomborg_section_3_3_2_global_trend.csv")

# Regional trends with verification
write_csv(regional_trends, "results/lomborg_section_3_3_2_regional_trends.csv")

cat("Results saved:\n")
cat("  - results/lomborg_section_3_3_2_global_trend.csv\n")
cat("  - results/lomborg_section_3_3_2_regional_trends.csv\n\n")

# ----------------------------------------------------------------------------
# PART 7: Visualizations
# ----------------------------------------------------------------------------

# Visualization 1: Global trend
p_global <- ggplot(gwis_global, aes(x = year, y = burned_area_ha / 1e6)) +
  geom_line(color = "gray70", linewidth = 1) +
  geom_point(size = 3, alpha = 0.7, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, color = "blue", fill = "grey80") +
  labs(
    title = "Global Burned Area: Long-Term Trend (2002-2023)",
    subtitle = sprintf("Mann-Kendall: $\tau$ = %.2f, p = %.2f | Significant DECLINING trend",
                      mk_global$tau[1], mk_global$sl[1]),
    x = "Year",
    y = "Burned Area (million hectares)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.subtitle = element_text(size = 10, face = "italic"),
    plot.title = element_text(face = "bold")
  )

ggsave("figures/hypothesis_tests/lomborg_3_3_2_global_trend.png",
       p_global, width = 12, height = 7, dpi = 300)

# Visualization 2: Regional trends (faceted)
regional_plot_data <- gwis_regional %>%
  filter(region %in% regions_to_analyze, year >= 2002, year <= 2023) %>%
  left_join(
    regional_trends %>% select(Region, Tau, P_Value, Significant),
    by = c("region" = "Region")
  ) %>%
  mutate(
    region_label = sprintf("%s\n$\tau$=%.2f, p=%.2f%s",
                          region, Tau, P_Value,
                          ifelse(Significant, "*", ""))
  )

p_regional <- ggplot(regional_plot_data, 
                     aes(x = year, y = burned_area_ha / 1e6)) +
  geom_line(linewidth = 0.8, color = "gray60") +
  geom_point(size = 2, alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, 
              aes(color = Significant, fill = Significant),
              linewidth = 1, alpha = 0.2) +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "gray50"),
                     labels = c("TRUE" = "Significant", "FALSE" = "Not significant")) +
  scale_fill_manual(values = c("TRUE" = "red", "FALSE" = "gray50"),
                    labels = c("TRUE" = "Significant", "FALSE" = "Not significant")) +
  facet_wrap(~region_label, scales = "free_y", ncol = 2) +
  labs(
    title = "Regional Burned Area Trends (2002-2023)",
    subtitle = "Regions with * show significant trends (p < 0.05)",
    x = "Year",
    y = "Burned Area (million hectares)",
    color = "Trend",
    fill = "Trend"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    plot.subtitle = element_text(size = 9, face = "italic"),
    plot.title = element_text(face = "bold"),
    strip.text = element_text(face = "bold", size = 9),
    legend.position = "bottom"
  )

ggsave("figures/hypothesis_tests/lomborg_3_3_2_regional_trends.png",
       p_regional, width = 14, height = 12, dpi = 300)

# Visualization 3: Forest plot of regional trends
p_forest <- ggplot(regional_trends, 
                   aes(x = Tau, y = reorder(Region, Tau))) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
  geom_point(aes(color = Significant), size = 4) +
  geom_segment(aes(x = 0, xend = Tau, y = Region, yend = Region,
                   color = Significant), linewidth = 1) +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "gray60"),
                     labels = c("TRUE" = "Significant", "FALSE" = "Not significant")) +
  labs(
    title = "Regional Fire Trends: Mann-Kendall Tau Values",
    subtitle = "Negative values indicate declining trends, positive indicate increasing",
    x = "Kendall's Tau ($\tau$)",
    y = NULL,
    color = "Significance (p < 0.05)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.subtitle = element_text(size = 10, face = "italic"),
    plot.title = element_text(face = "bold"),
    legend.position = "bottom"
  )

ggsave("figures/hypothesis_tests/lomborg_3_3_2_regional_forest_plot.png",
       p_forest, width = 10, height = 7, dpi = 300)

cat("Visualizations saved:\n")
cat("  - figures/hypothesis_tests/lomborg_3_3_2_global_trend.png\n")
cat("  - figures/hypothesis_tests/lomborg_3_3_2_regional_trends.png\n")
cat("  - figures/hypothesis_tests/lomborg_3_3_2_regional_forest_plot.png\n\n")

cat("  Section 3.3.2 complete\n\n")

# ----------------------------------------------------------------------------
# FINAL SUMMARY
# ----------------------------------------------------------------------------

cat("========================================\n")
cat("SECTION 3.3.2 SUMMARY\n")
cat("========================================\n\n")

cat(sprintf("Global trend: $\tau$ = %.2f, p = %.3f → %s\n",
            mk_global$tau[1], mk_global$sl[1],
            ifelse(mk_global$sl[1] < 0.05, "SIGNIFICANT", "Not significant")))

cat(sprintf("\nRegions with significant DECLINES: %d\n", 
            sum(regional_trends$Significant & regional_trends$Tau < 0, na.rm = TRUE)))
if (nrow(declining_regions) > 0) {
  for (i in 1:nrow(declining_regions)) {
    cat(sprintf("  - %s ($\tau$ = %.2f)\n", declining_regions$Region[i], declining_regions$Tau[i]))
  }
}

cat(sprintf("\nRegions with NO significant trend: %d\n",
            sum(!regional_trends$Significant, na.rm = TRUE)))
if (nrow(nonsig_regions) > 0) {
  for (i in 1:nrow(nonsig_regions)) {
    cat(sprintf("  - %s ($\tau$ = %.2f, p = %.2f)\n", 
                nonsig_regions$Region[i], nonsig_regions$Tau[i], nonsig_regions$P_Value[i]))
  }
}


```

---

# Final Summary

## Comprehensive Summary of All Tests

```{r final-summary}
# ============================================================================
# FINAL COMPREHENSIVE SUMMARY
# ============================================================================

cat("\n")
cat("########################################################################\n")
cat("#              COMPREHENSIVE SUMMARY - ALL HYPOTHESIS TESTS            #\n")
cat("########################################################################\n\n")

# ----------------------------------------------------------------------------
# DENLEY'S ARTICLE (Section 3.1)
# ----------------------------------------------------------------------------

cat("SECTION 3.1: DENLEY'S ARTICLE\n")
cat(strrep("=", 72), "\n\n")

denley_summary <- tibble(
  Section = c("3.1.1", "3.1.2", "3.1.3"),
  Test = c("Fire Frequency", "Area Burned", "Temp-Fire Correlation"),
  Statistic = c(
    sprintf("$\tau$ = %.2f", mk_fires$tau[1]),
    sprintf("$\tau$ = %.2f", mk_area$tau[1]),
    sprintf("$\rho$ = %.3f", spearman_test$estimate)
  ),
  P_Value = c(mk_fires$sl[1], mk_area$sl[1], spearman_test$p.value),
  Result = c(
    ifelse(mk_fires$sl[1] < 0.05 & mk_fires$tau[1] < 0, "  SUPPORTED", " NOT SUPPORTED"),
    ifelse(mk_area$sl[1] < 0.05 & mk_area$tau[1] < 0, "  SUPPORTED", " NOT SUPPORTED"),
    ifelse(spearman_test$p.value < 0.05 & spearman_test$estimate < 0, "  SUPPORTED", " NOT SUPPORTED")
  )
)

print(denley_summary, n = Inf)
cat("\nConclusion: Fire frequency declined Passed, but area burned stable Failed , and no opposite temp-fire pattern Failed \n\n")

# ----------------------------------------------------------------------------
# SANKEY'S ARTICLE (Section 3.2)
# ----------------------------------------------------------------------------

cat("SECTION 3.2: SANKEY'S ARTICLE\n")
cat(strrep("=", 72), "\n\n")

sankey_summary <- tibble(
  Section = c("3.2.1", "3.2.1", "3.2.2"),
  Test = c("Human Proportion", "Human Count Trend", "Area by Cause"),
  Statistic = c(
    sprintf("%.1f%%", overall_props$Mean_Prop_Human * 100),
    sprintf("$\tau$ = %.2f", mk_human_count$tau[1]),
    sprintf("W = %.1f", mw_test_area$statistic)
  ),
  P_Value = c(NA, mk_human_count$sl[1], mw_test_area$p.value),
  Result = c(
    "  ACCURATE",
    ifelse(mk_human_count$sl[1] < 0.05 & mk_human_count$tau[1] > 0, "  SUPPORTED", " NOT SUPPORTED"),
    "  SIGNIFICANT DIFF"
  )
)

print(sankey_summary, n = Inf)
cat("\nConclusion: Human proportion stable (~49%) Passed , counts DECLINING Failed, lightning burns 11× more area Passed \n\n")

# ----------------------------------------------------------------------------
# LOMBORG'S ARTICLE (Section 3.3)
# ----------------------------------------------------------------------------

cat("SECTION 3.3: LOMBORG'S ARTICLE\n")
cat(strrep("=", 72), "\n\n")

# Calculate baselines
baseline_10yr <- gwis_global %>% 
  filter(year >= 2014, year <= 2023) %>% 
  summarise(Mean = mean(burned_area_ha)) %>% pull(Mean)

baseline_20yr <- gwis_global %>% 
  filter(year >= 2004, year <= 2023) %>% 
  summarise(Mean = mean(burned_area_ha)) %>% pull(Mean)

baseline_full <- gwis_global %>% 
  filter(year >= 2002, year <= 2023) %>% 
  summarise(Mean = mean(burned_area_ha)) %>% pull(Mean)

value_2023 <- gwis_global %>% filter(year == 2023) %>% pull(burned_area_ha)

lomborg_summary <- tibble(
  Section = c("3.3.1", "3.3.1", "3.3.1", "3.3.2"),
  Test = c("10-yr baseline", "20-yr baseline", "Full baseline", "Global trend"),
  Statistic = c(
    sprintf("%+.1f%%", ((value_2023 - baseline_10yr) / baseline_10yr) * 100),
    sprintf("%+.1f%%", ((value_2023 - baseline_20yr) / baseline_20yr) * 100),
    sprintf("%+.1f%%", ((value_2023 - baseline_full) / baseline_full) * 100),
    sprintf("$\tau$ = %.2f", mk_global$tau[1])
  ),
  P_Value = c(NA, NA, NA, mk_global$sl[1]),
  Result = c(
    ifelse(value_2023 < baseline_10yr, "Below avg", "Above avg"),
    ifelse(value_2023 < baseline_20yr, "Below avg", "Above avg"),
    ifelse(value_2023 < baseline_full, "Below avg", "Below avg"),
    "  DECLINING"
  )
)

print(lomborg_summary, n = Inf)
cat("\nConclusion: Baseline-dependent (reverses with 10yr), global decline driven by Africa Passed \n\n")

# ----------------------------------------------------------------------------
# MASTER SUMMARY
# ----------------------------------------------------------------------------

cat("MASTER SUMMARY: KEY FINDINGS\n")
cat(strrep("=", 72), "\n\n")

cat("1. CANADIAN FIRES:\n")
cat("   • Fire counts DECLINING ($\tau$ = -0.42, p < 0.001)  \n")
cat("   • Area burned NO TREND (p = 0.27) \n")
cat("   • Mean fire size UP 60% (452→723 ha) \n\n")

cat("2. HUMAN VS CLIMATE:\n")
cat("   • Human proportion STABLE at ~49% (p = 0.57)  \n")
cat("   • Human counts DECLINING ($\tau$ = -0.55, p < 0.001)  \n")
cat("   • Lightning burns 11× more area (p < 0.001)  \n")
cat("   • Temp-fire correlation near ZERO ($\rho$ = 0.006)  \n\n")

cat("3. GLOBAL CONTEXT:\n")
cat("   • Global fires DECLINING ($\tau$ = -0.35, p = 0.02)  \n")
cat("   • Driven by Africa ($\tau$ = -0.58, p < 0.001)  \n")
cat("   • North America NO TREND ($\tau$ = 0.18, p = 0.20)   \n")
cat("   • Baseline choice affects conclusion \n\n")

# Save master summary
master_summary <- bind_rows(
  denley_summary %>% mutate(Article = "Denley"),
  sankey_summary %>% mutate(Article = "Sankey"),
  lomborg_summary %>% mutate(Article = "Lomborg")
)

write_csv(master_summary, "results/MASTER_SUMMARY.csv")

cat("  Master summary saved to: results/MASTER_SUMMARY.csv\n\n")

cat("########################################################################\n")
cat("#                        ANALYSIS COMPLETE                             #\n")
cat("########################################################################\n\n")
```


# Session Information

```{r final-session-info}
sessionInfo()
```
